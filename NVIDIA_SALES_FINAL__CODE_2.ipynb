{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHf-NFGz3_rL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive to access files from it\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/NVIDIA_MASTER.csv')\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3921259"
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_file_path = '/content/drive/MyDrive/NVIDIA Project Files Team 4.zip'\n",
        "extraction_path = '/content/drive/MyDrive/NVIDIA_unzipped_files'\n",
        "\n",
        "# Create the directory for extraction if it doesn't exist\n",
        "os.makedirs(extraction_path, exist_ok=True)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extraction_path)\n",
        "\n",
        "print(f\"'{zip_file_path}' unzipped to '{extraction_path}'\")\n",
        "print(f\"Contents of '{extraction_path}':\")\n",
        "print(os.listdir(extraction_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ab27864"
      },
      "source": [
        "import os\n",
        "\n",
        "# List the contents of your Google Drive's root folder\n",
        "print(os.listdir('/content/drive/MyDrive'))\n",
        "\n",
        "# If your file is in a subfolder, you might need to adjust the path:\n",
        "# print(os.listdir('/content/drive/MyDrive/Your_Folder_Name'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna()\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "eA9CdkzC4JMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "825e14e4"
      },
      "source": [
        "df = df.drop('Product Name', axis=1)\n",
        "display(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aggregated_df = df.groupby(['Region', 'Product Category', 'Date']).agg({\n",
        "    'Sales Revenue (USD)': 'sum',\n",
        "    'Units Sold': 'sum',\n",
        "    'Customer Satisfaction': 'mean',\n",
        "    'Marketing Spend (USD)': 'sum',\n",
        "    'Discount Percentage (%)': 'mean',\n",
        "    'Return Rate (%)': 'mean',\n",
        "    'AI/ML Adoption Rate (%)': 'mean',\n",
        "    'Ad Campaign Effectiveness': 'mean',\n",
        "    'Customer Retention Rate (%)': 'mean',\n",
        "    'Market Share (%)': 'mean',\n",
        "    'Total_Revenue': 'sum'\n",
        "}).reset_index()\n",
        "\n",
        "display(aggregated_df.head())"
      ],
      "metadata": {
        "id": "Xctdzk6R48ID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by 'Region' and 'Product Category' and check for unique dates\n",
        "is_unique_date = aggregated_df.groupby(['Region', 'Product Category'])['Date'].nunique() == aggregated_df.groupby(['Region', 'Product Category'])['Date'].size()\n",
        "\n",
        "# Check if all combinations have unique dates\n",
        "if is_unique_date.all():\n",
        "    print(\"All Region and Product Category combinations have unique dates.\")\n",
        "else:\n",
        "    print(\"Some Region and Product Category combinations have duplicate dates.\")\n",
        "    # Optionally, display the combinations with duplicate dates\n",
        "    print(\"Combinations with duplicate dates:\")\n",
        "    print(is_unique_date[~is_unique_date])"
      ],
      "metadata": {
        "id": "C2PhpHEY5Zv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create a directory to save the CSV files in Google Drive\n",
        "output_dir = '/content/drive/MyDrive/NVIDIA/aggregated_dataframes'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Get unique combinations of Region and Product Category\n",
        "unique_combinations = aggregated_df.groupby(['Region', 'Product Category']).groups.keys()\n",
        "\n",
        "# Iterate through unique combinations and save each as a CSV\n",
        "for region, product_category in unique_combinations:\n",
        "    # Filter the aggregated_df for the current combination\n",
        "    filtered_df = aggregated_df[(aggregated_df['Region'] == region) & (aggregated_df['Product Category'] == product_category)]\n",
        "\n",
        "    # Create a filename based on the combination\n",
        "    filename = f\"{region}_{product_category}.csv\".replace(\" \", \"_\").replace(\"/\", \"_\") # Replace spaces and slashes for valid filenames\n",
        "    filepath = os.path.join(output_dir, filename)\n",
        "\n",
        "    # Save the filtered dataframe to CSV\n",
        "    filtered_df.to_csv(filepath, index=False)\n",
        "    print(f\"Saved {filename}\")\n",
        "\n",
        "print(f\"\\nAll aggregated dataframes saved to {output_dir}\")"
      ],
      "metadata": {
        "id": "bZGeD2SC5mZR",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85b0ed77"
      },
      "source": [
        "# Task\n",
        "Analyze time series data from multiple CSV files located in \"/content/drive/MyDrive/NVIDIA/aggregated_dataframes\", checking for seasonality, plotting ACF and PACF, performing White Noise and stationarity tests, and summarizing the results in a table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7220a4b3"
      },
      "source": [
        "## List files\n",
        "\n",
        "### Subtask:\n",
        "Get the list of all CSV files in the specified directory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f10b30b7"
      },
      "source": [
        "**Reasoning**:\n",
        "Get the list of all CSV files in the specified directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e69a385e"
      },
      "source": [
        "import os\n",
        "\n",
        "output_dir = '/content/drive/MyDrive/NVIDIA/aggregated_dataframes'\n",
        "csv_files = [f for f in os.listdir(output_dir) if f.endswith('.csv')]\n",
        "print(csv_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e6fc32d"
      },
      "source": [
        "## Iterate and load\n",
        "\n",
        "### Subtask:\n",
        "Loop through each CSV file, load it into a pandas DataFrame, and convert the 'Date' column to datetime objects.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80951a72"
      },
      "source": [
        "**Reasoning**:\n",
        "Loop through each CSV file, load it into a pandas DataFrame, convert the 'Date' column to datetime objects, and store the preprocessed DataFrames in a dictionary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40ea58ea"
      },
      "source": [
        "dataframes = {}\n",
        "\n",
        "for filename in csv_files:\n",
        "    filepath = os.path.join(output_dir, filename)\n",
        "    df_temp = pd.read_csv(filepath)\n",
        "    df_temp['Date'] = pd.to_datetime(df_temp['Date'])\n",
        "    key_name = filename.replace('.csv', '')\n",
        "    dataframes[key_name] = df_temp\n",
        "\n",
        "# Display the keys of the dictionary and the head of one of the dataframes as a check\n",
        "print(\"Loaded dataframes:\")\n",
        "print(dataframes.keys())\n",
        "print(\"\\nHead of one dataframe:\")\n",
        "display(list(dataframes.values())[0].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "493c2374"
      },
      "source": [
        "## Perform time series analysis\n",
        "\n",
        "### Subtask:\n",
        "For each DataFrame, perform the requested time series analysis:\n",
        "- Check for seasonality.\n",
        "- Plot ACF and PACF.\n",
        "- Perform White Noise test.\n",
        "- Perform Stationarity test (e.g., Augmented Dickey-Fuller test).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "389891e8"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through each dataframe in the `dataframes` dictionary and perform the requested time series analysis steps (seasonality check, ACF/PACF plots, White Noise test, and Stationarity test) on the 'Sales Revenue (USD)' column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eaca00f"
      },
      "source": [
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, df in dataframes.items():\n",
        "    print(f\"Analyzing {name}:\")\n",
        "    ts = df['Sales Revenue (USD)']\n",
        "\n",
        "    # 1. Visual inspection for seasonality (Plotting the time series)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(df['Date'], ts)\n",
        "    plt.title(f'{name} - Sales Revenue (USD) Over Time')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Sales Revenue (USD)')\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Plot ACF and PACF\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 4))\n",
        "    plot_acf(ts, ax=axes[0])\n",
        "    plot_pacf(ts, ax=axes[1])\n",
        "    axes[0].set_title(f'{name} - ACF')\n",
        "    axes[1].set_title(f'{name} - PACF')\n",
        "    plt.show()\n",
        "\n",
        "    # 3. Perform White Noise test (Ljung-Box test)\n",
        "    ljung_box_result = acorr_ljungbox(ts, lags=[10], return_df=True)\n",
        "    ljung_box_pvalue = ljung_box_result['lb_pvalue'].iloc[0]\n",
        "    print(f\"Ljung-Box test p-value: {ljung_box_pvalue:.4f}\")\n",
        "    white_noise_status = \"Not White Noise\" if ljung_box_pvalue < 0.05 else \"White Noise\"\n",
        "    print(f\"Conclusion: The time series is likely {white_noise_status}.\\n\")\n",
        "\n",
        "    # 4. Perform Stationarity test (Augmented Dickey-Fuller test)\n",
        "    adf_result = adfuller(ts)\n",
        "    adf_statistic = adf_result[0]\n",
        "    adf_pvalue = adf_result[1]\n",
        "    print(f\"ADF Statistic: {adf_statistic:.4f}\")\n",
        "    print(f\"ADF p-value: {adf_pvalue:.4f}\")\n",
        "    stationarity_status = \"Stationary\" if adf_pvalue < 0.05 else \"Non-Stationary\"\n",
        "    print(f\"Conclusion: The time series is likely {stationarity_status}.\\n\")\n",
        "\n",
        "    results[name] = {\n",
        "        'Ljung-Box p-value': ljung_box_pvalue,\n",
        "        'White Noise Conclusion': white_noise_status,\n",
        "        'ADF Statistic': adf_statistic,\n",
        "        'ADF p-value': adf_pvalue,\n",
        "        'Stationarity Conclusion': stationarity_status\n",
        "    }\n",
        "\n",
        "# Summarize results in a table\n",
        "results_df = pd.DataFrame.from_dict(results, orient='index')\n",
        "print(\"Summary of Time Series Analysis Results:\")\n",
        "display(results_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set plot style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, df in dataframes.items():\n",
        "    print(f\"Analyzing time series for {name}...\")\n",
        "    df_temp = df.copy()\n",
        "    df_temp['Date'] = pd.to_datetime(df_temp['Date'])\n",
        "    df_temp = df_temp.set_index('Date').sort_index()\n",
        "\n",
        "    # Select the target variable and resample to a consistent frequency (e.g., daily 'D')\n",
        "    # Aggregate by summing sales revenue for each day if multiple entries exist for the same date\n",
        "    ts = df_temp['Sales Revenue (USD)'].resample('D').sum().fillna(method='ffill') # Forward fill missing daily data\n",
        "\n",
        "    # Decompose time series into trend, seasonal, and residual components\n",
        "    # Adjust model based on assumption (additive or multiplicative) - additive is often a good start\n",
        "    try:\n",
        "        decomposition = seasonal_decompose(ts, model='additive')\n",
        "\n",
        "        # Plot the decomposed time series components\n",
        "        fig, axes = plt.subplots(4, 1, figsize=(12, 10), sharex=True)\n",
        "        axes[0].plot(ts, label='Original')\n",
        "        axes[0].set_title(f'{name} - Original Time Series')\n",
        "        axes[0].legend()\n",
        "\n",
        "        axes[1].plot(decomposition.trend, label='Trend')\n",
        "        axes[1].set_title(f'{name} - Trend Component')\n",
        "        axes[1].legend()\n",
        "\n",
        "        axes[2].plot(decomposition.seasonal, label='Seasonal')\n",
        "        axes[2].set_title(f'{name} - Seasonal Component')\n",
        "        axes[2].legend()\n",
        "\n",
        "        axes[3].plot(decomposition.resid, label='Residuals')\n",
        "        axes[3].set_title(f'{name} - Residual Component')\n",
        "        axes[3].legend()\n",
        "\n",
        "        plt.xlabel('Date')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Analyze residuals: ACF and PACF plots\n",
        "        print(f\"Analyzing residuals for {name}...\")\n",
        "        residuals = decomposition.resid.dropna() # Drop NaNs for residual analysis\n",
        "\n",
        "        if not residuals.empty:\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "            plot_acf(residuals, ax=axes[0], lags=min(40, len(residuals)//2 - 1)) # Plot ACF of residuals\n",
        "            axes[0].set_title(f'{name} - Residuals ACF')\n",
        "            plot_pacf(residuals, ax=axes[1], lags=min(40, len(residuals)//2 - 1)) # Plot PACF of residuals\n",
        "            axes[1].set_title(f'{name} - Residuals PACF')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Ljung-Box test for white noise in residuals\n",
        "            # Perform test on a range of lags\n",
        "            ljung_box_result = acorr_ljungbox(residuals, lags=[10], return_df=True)\n",
        "            ljung_box_pvalue = ljung_box_result.iloc[0, 1]\n",
        "\n",
        "            white_noise_conclusion = \"White Noise\" if ljung_box_pvalue > 0.05 else \"Not White Noise\"\n",
        "\n",
        "            print(f\"Ljung-Box test p-value for {name}: {ljung_box_pvalue:.4f}\")\n",
        "            print(f\"Conclusion: Residuals are likely {white_noise_conclusion}\")\n",
        "\n",
        "            results[name] = {\n",
        "                'Ljung-Box p-value': ljung_box_pvalue,\n",
        "                'White Noise Conclusion': white_noise_conclusion\n",
        "            }\n",
        "        else:\n",
        "            print(f\"Residuals are empty for {name}, skipping residual analysis.\")\n",
        "            results[name] = {\n",
        "                'Ljung-Box p-value': np.nan,\n",
        "                'White Noise Conclusion': 'N/A (Empty Residuals)'\n",
        "            }\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Could not perform time series decomposition for {name}: {e}\")\n",
        "        results[name] = {\n",
        "            'Ljung-Box p-value': np.nan,\n",
        "            'White Noise Conclusion': f'Error during decomposition: {e}'\n",
        "        }\n",
        "\n",
        "\n",
        "print(\"\\nTime series analysis complete for all dataframes.\")"
      ],
      "metadata": {
        "id": "MkqJboM5kTUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Robustness and Stability Check"
      ],
      "metadata": {
        "id": "VHvH6Mtzm6Yz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "414d805d"
      },
      "source": [
        "## Present summary\n",
        "\n",
        "### Subtask:\n",
        "Display the summarized results for each file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4730d253"
      },
      "source": [
        "**Reasoning**:\n",
        "Display the `results_df` DataFrame which contains the summary of the time series analysis for each file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adc00476"
      },
      "source": [
        "display(results_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dda3cf4"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   Most of the time series analyzed were found to be White Noise according to the Ljung-Box test (p-value > 0.05), except for \"Middle_East_OEM\" and \"North_America_Data_Center\" which were identified as \"Not White Noise\".\n",
        "*   All time series were concluded to be Stationary based on the Augmented Dickey-Fuller (ADF) test results (p-value < 0.05 for all).\n",
        "*   The analysis involved loading 24 CSV files, converting their 'Date' column to datetime objects, plotting time series, ACF, and PACF for each, and performing Ljung-Box and ADF tests.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Given that most series are White Noise and all are stationary, simple forecasting models might be sufficient, but the \"Not White Noise\" series (\"Middle_East_OEM\" and \"North_America_Data_Center\") might benefit from specific time series models like ARIMA or SARIMA, provided the visual inspection of plots suggests specific patterns not captured by the tests alone.\n",
        "*   Investigate the underlying reasons why \"Middle_East_OEM\" and \"North_America_Data_Center\" series are not white noise, which could indicate specific patterns or external factors influencing their sales revenue.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QwCQs3Gs938E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4adf72ea"
      },
      "source": [
        "# Task\n",
        "Load the aggregated dataframes from the CSV files located in \"/content/drive/MyDrive/NVIDIA/aggregated_dataframes\", train and test an XGBoost model using 'Sales Revenue' as the dependent variable to forecast sales revenue for the next 1 year for each dataframe, calculate confidence intervals for the forecasts, and visualize the historical data, forecasted sales revenue, and confidence intervals for each dataframe in a professional presentation-ready format with legends."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2ba7f86"
      },
      "source": [
        "## Load data\n",
        "\n",
        "### Subtask:\n",
        "Load the aggregated dataframes from the previously saved CSV files.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0422924d"
      },
      "source": [
        "**Reasoning**:\n",
        "Loop through each CSV file, load it into a pandas DataFrame, convert the 'Date' column to datetime objects, and store the preprocessed DataFrames in a dictionary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54595483"
      },
      "source": [
        "dataframes = {}\n",
        "\n",
        "for filename in csv_files:\n",
        "    filepath = os.path.join(output_dir, filename)\n",
        "    df_temp = pd.read_csv(filepath)\n",
        "    df_temp['Date'] = pd.to_datetime(df_temp['Date'])\n",
        "    key_name = filename.replace('.csv', '')\n",
        "    dataframes[key_name] = df_temp\n",
        "\n",
        "# Display the keys of the dictionary and the head of one of the dataframes as a check\n",
        "print(\"Loaded dataframes:\")\n",
        "print(dataframes.keys())\n",
        "display(list(dataframes.values())[0].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62217a2c"
      },
      "source": [
        "## Prepare data for forecasting\n",
        "\n",
        "### Subtask:\n",
        "For each dataframe, prepare the data for time series forecasting by creating features and splitting the data into training and testing sets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f36a4356"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through each dataframe, set the 'Date' column as the index, define features and target, and split the data into training and testing sets based on the date threshold.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4975d444"
      },
      "source": [
        "forecast_data = {}\n",
        "date_threshold = '2019-09-23'\n",
        "\n",
        "for name, df in dataframes.items():\n",
        "    df = df.set_index('Date')\n",
        "\n",
        "    target = df['Sales Revenue (USD)']\n",
        "    features = df[['Marketing Spend (USD)', 'Ad Campaign Effectiveness']]\n",
        "\n",
        "    X_train = features[features.index <= date_threshold]\n",
        "    X_test = features[features.index > date_threshold]\n",
        "    y_train = target[target.index <= date_threshold]\n",
        "    y_test = target[target.index > date_threshold]\n",
        "\n",
        "    forecast_data[name] = {\n",
        "        'X_train': X_train,\n",
        "        'X_test': X_test,\n",
        "        'y_train': y_train,\n",
        "        'y_test': y_test,\n",
        "        'date_threshold': date_threshold\n",
        "    }\n",
        "\n",
        "# Display the keys of the new dictionary and the head of one of the training sets as a check\n",
        "print(\"Prepared data for forecasting:\")\n",
        "print(forecast_data.keys())\n",
        "print(\"\\nHead of one training set (features):\")\n",
        "display(list(forecast_data.values())[0]['X_train'].head())\n",
        "print(\"\\nHead of one training set (target):\")\n",
        "display(list(forecast_data.values())[0]['y_train'].head())\n",
        "print(\"\\nHead of one testing set (features):\")\n",
        "display(list(forecast_data.values())[0]['X_test'].head())\n",
        "print(\"\\nHead of one testing set (target):\")\n",
        "display(list(forecast_data.values())[0]['y_test'].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c58459b1"
      },
      "source": [
        "## Train xgboost model\n",
        "\n",
        "### Subtask:\n",
        "Train an XGBoost model on the training data for each dataframe.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcfd4da8"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through each dataframe in the `forecast_data` dictionary, train an XGBoost model on the training data, and store the trained model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "070aa417"
      },
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "models = {}\n",
        "\n",
        "for name, data in forecast_data.items():\n",
        "    print(f\"Training XGBoost model for {name}...\")\n",
        "    X_train = data['X_train']\n",
        "    y_train = data['y_train']\n",
        "\n",
        "    model = XGBRegressor(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    models[name] = model\n",
        "    print(f\"Finished training for {name}\")\n",
        "\n",
        "print(\"\\nAll XGBoost models trained.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a85ca7cb"
      },
      "source": [
        "## Forecast future sales\n",
        "\n",
        "### Subtask:\n",
        "Use the trained XGBoost model to forecast sales revenue for the next 1 year for each dataframe.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6529aa1"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through each trained model and its corresponding test features to generate sales revenue forecasts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "105e39f6"
      },
      "source": [
        "forecasts = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"Generating forecasts for {name}...\")\n",
        "    X_test = forecast_data[name]['X_test']\n",
        "    forecast = model.predict(X_test)\n",
        "    forecasts[name] = pd.Series(forecast, index=X_test.index)\n",
        "    print(f\"Finished forecasting for {name}\")\n",
        "\n",
        "print(\"\\nAll forecasts generated.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca22c049"
      },
      "source": [
        "## Calculate confidence intervals\n",
        "\n",
        "### Subtask:\n",
        "Calculate confidence intervals for the forecasted sales revenue.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83386f7f"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the trained models and forecast data, calculate residuals, their standard deviation, and then compute the confidence intervals for the forecasts based on a 95% confidence level. Store these confidence intervals.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "530d7312"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "\n",
        "confidence_intervals = {}\n",
        "confidence_level = 0.95\n",
        "z_score = norm.ppf((1 + confidence_level) / 2) # Z-score for 95% confidence\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"Calculating confidence intervals for {name}...\")\n",
        "    X_train = forecast_data[name]['X_train']\n",
        "    y_train = forecast_data[name]['y_train']\n",
        "    X_test = forecast_data[name]['X_test']\n",
        "\n",
        "    # Calculate residuals on the training data\n",
        "    residuals = y_train - model.predict(X_train)\n",
        "\n",
        "    # Calculate the standard deviation of the residuals\n",
        "    residual_std = residuals.std()\n",
        "\n",
        "    # Calculate the margin of error\n",
        "    margin_of_error = z_score * residual_std\n",
        "\n",
        "    # Calculate the lower and upper bounds of the confidence interval for the forecasts\n",
        "    lower_bound = forecasts[name] - margin_of_error\n",
        "    upper_bound = forecasts[name] + margin_of_error\n",
        "\n",
        "    confidence_intervals[name] = pd.DataFrame({\n",
        "        'lower_bound': lower_bound,\n",
        "        'upper_bound': upper_bound\n",
        "    }, index=forecasts[name].index)\n",
        "\n",
        "    print(f\"Finished calculating confidence intervals for {name}\")\n",
        "\n",
        "print(\"\\nConfidence intervals for all forecasts calculated.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a1c9269"
      },
      "source": [
        "## Visualize forecasts\n",
        "\n",
        "### Subtask:\n",
        "For each dataframe, visualize the historical data, the forecasted sales revenue, and the confidence interval, ensuring the plots are suitable for a presentation with legends.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2009fe8f"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through each dataframe and plot the historical data, forecasted sales, and confidence intervals.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9e1aaf7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for name in dataframes.keys():\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(forecast_data[name]['y_train'].index, forecast_data[name]['y_train'], label='Historical Data')\n",
        "    plt.plot(forecasts[name].index, forecasts[name], label='Forecast')\n",
        "    plt.plot(confidence_intervals[name].index, confidence_intervals[name]['upper_bound'], label='Upper Bound', linestyle='--')\n",
        "    plt.plot(confidence_intervals[name].index, confidence_intervals[name]['lower_bound'], label='Lower Bound', linestyle='--')\n",
        "    plt.title(f'Sales Revenue Forecast for {name}')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Sales Revenue (USD)')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a3b7c58"
      },
      "source": [
        "## Summarize results\n",
        "\n",
        "### Subtask:\n",
        "Summarize the forecasting results, including model performance metrics and key observations for each dataframe.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "582ce84c"
      },
      "source": [
        "**Reasoning**:\n",
        "Calculate and display the MAE and RMSE for each dataframe's forecasts and summarize the results in a DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7mqeYglBLOY"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "results_summary = {}\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    \"\"\"Calculates Mean Absolute Percentage Error (MAPE).\"\"\"\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "for name in forecasts.keys():\n",
        "    y_true = forecast_data[name]['y_test']\n",
        "    y_pred = forecasts[name]\n",
        "\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
        "\n",
        "\n",
        "    results_summary[name] = {\n",
        "        'MAE': mae,\n",
        "        'RMSE': rmse,\n",
        "        'MAPE': mape\n",
        "    }\n",
        "\n",
        "summary_df = pd.DataFrame.from_dict(results_summary, orient='index')\n",
        "\n",
        "print(\"Summary of Forecasting Performance (MAE, RMSE, and MAPE):\")\n",
        "display(summary_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "087ec802"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The data was successfully loaded from 24 CSV files, each representing aggregated data for a specific region and product category.\n",
        "*   For each dataset, features ('Marketing Spend (USD)' and 'Ad Campaign Effectiveness') and the target variable ('Sales Revenue (USD)') were identified.\n",
        "*   The data was split into training (up to '2019-09-23') and testing sets (after '2019-09-23') for each dataset.\n",
        "*   An XGBoost Regressor model was trained on the training data for each of the 24 datasets.\n",
        "*   Sales revenue forecasts for the next year were generated using the trained models on the testing data.\n",
        "*   95% confidence intervals for the forecasts were calculated based on the residuals of the models on the training data.\n",
        "*   Visualizations were created for each dataset, displaying historical data, forecasted sales revenue, and the calculated confidence intervals.\n",
        "*   Model performance was evaluated using Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) on the test sets for each dataset.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Analyze the MAE and RMSE values across different regions and product categories to identify which combinations have the most accurate forecasts and which require further model tuning or feature engineering.\n",
        "*   Investigate the periods where the actual sales fall outside the 95% confidence intervals in the visualizations to understand potential drivers of unexpected sales fluctuations not captured by the current features.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualize the first tree for each model\n",
        "for name, model in models.items():\n",
        "    print(f\"Visualizing the first tree for {name}...\")\n",
        "    fig, ax = plt.subplots(figsize=(200, 100))\n",
        "    plot_tree(model, tree_idx=0, ax=ax) # Changed num_trees to tree_idx\n",
        "    ax.set_title(f\"First Tree of XGBoost Model for {name}\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "LM_EH1v5G_r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5544a90"
      },
      "source": [
        "## Present summary of forecasting results\n",
        "\n",
        "### Subtask:\n",
        "Display the summarized forecasting performance metrics for each dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceb2fba8"
      },
      "source": [
        "**Reasoning**:\n",
        "Display the `summary_df` DataFrame which contains the MAE, RMSE, and MAPE for each dataframe's sales revenue forecast. This table provides a clear overview of the model's performance across different regions and product categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5929a236"
      },
      "source": [
        "display(summary_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab1fb90d"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "* The data was successfully loaded from 24 CSV files, each representing aggregated data for a specific region and product category.\n",
        "* For each dataset, features ('Marketing Spend (USD)' and 'Ad Campaign Effectiveness') and the target variable ('Sales Revenue (USD)') were identified.\n",
        "* The data was split into training (up to '2019-09-23') and testing sets (after '2019-09-23') for each dataset.\n",
        "* An XGBoost Regressor model was trained on the training data for each of the 24 datasets.\n",
        "* Sales revenue forecasts for the next year were generated using the trained models on the testing data.\n",
        "* 95% confidence intervals for the forecasts were calculated based on the residuals of the models on the training data.\n",
        "* Visualizations were created for each dataset, displaying historical data, forecasted sales revenue, and the calculated confidence intervals, including the visualized decision trees for each model.\n",
        "* Model performance was evaluated using Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE) on the test sets for each dataset.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "* **Compare performance across datasets:** Analyze the MAE, RMSE, and MAPE values in the summary table (`summary_df`) to identify which Region and Product Category combinations have the most accurate forecasts (lower MAE, RMSE, and MAPE) and which have higher errors, indicating potential areas for improvement.\n",
        "* **Investigate high error datasets:** For datasets with high MAE, RMSE, and/or MAPE, explore the time series plots and the underlying data to understand if there are specific patterns, outliers, or external factors that the current features are not capturing.\n",
        "* **Feature Engineering:** Consider creating additional relevant features that might improve the model's ability to capture patterns in the data, such as lag features, rolling statistics, or external economic indicators.\n",
        "* **Model Tuning:** Experiment with different hyperparameters for the XGBoost model to potentially improve performance.\n",
        "* **Explore other models:** If XGBoost performance is not satisfactory, consider exploring other time series forecasting models like ARIMA, SARIMA, or other machine learning models suitable for sequential data.\n",
        "* **Refine Confidence Intervals:** The current confidence intervals are based on the standard deviation of the residuals from the training data. For a more robust analysis, consider exploring methods for calculating confidence intervals specifically designed for time series forecasts or tree-based models.\n",
        "* **Domain Knowledge:** Incorporate domain knowledge about the specific regions and product categories to interpret the forecasting results and identify potential drivers of sales revenue that were not included in the model.\n",
        "\n",
        "**Finish task**: Review the analysis and findings to draw conclusions about the sales revenue forecasting for the different regions and product categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab6f80a2"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "* The data was successfully loaded from 24 CSV files, each representing aggregated data for a specific region and product category.\n",
        "* For each dataset, features ('Marketing Spend (USD)' and 'Ad Campaign Effectiveness') and the target variable ('Sales Revenue (USD)') were identified.\n",
        "* The data was split into training (up to '2019-09-23') and testing sets (after '2019-09-23') for each dataset.\n",
        "* An XGBoost Regressor model was trained on the training data for each of the 24 datasets.\n",
        "* Sales revenue forecasts for the next year were generated using the trained models on the testing data.\n",
        "* 95% confidence intervals for the forecasts were calculated based on the residuals of the models on the training data.\n",
        "* Visualizations were created for each dataset, displaying historical data, forecasted sales revenue, and the calculated confidence intervals.\n",
        "* Model performance was evaluated using Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE) on the test sets for each dataset.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "* **Compare performance across datasets:** Analyze the MAE, RMSE, and MAPE values in the summary table (`summary_df`) to identify which Region and Product Category combinations have the most accurate forecasts (lower MAE, RMSE, and MAPE) and which have higher errors, indicating potential areas for improvement.\n",
        "* **Investigate high error datasets:** For datasets with high MAE, RMSE, and/or MAPE, explore the time series plots and the underlying data to understand if there are specific patterns, outliers, or external factors that the current features are not capturing.\n",
        "* **Feature Engineering:** Consider creating additional relevant features that might improve the model's ability to capture patterns in the data, such as lag features, rolling statistics, or external economic indicators.\n",
        "* **Model Tuning:** Experiment with different hyperparameters for the XGBoost model to potentially improve performance.\n",
        "* **Explore other models:** If XGBoost performance is not satisfactory, consider exploring other time series forecasting models like ARIMA, SARIMA, or other machine learning models suitable for sequential data.\n",
        "* **Refine Confidence Intervals:** The current confidence intervals are based on the standard deviation of the residuals from the training data. For a more robust analysis, consider exploring methods for calculating confidence intervals specifically designed for time series forecasts or tree-based models.\n",
        "* **Domain Knowledge:** Incorporate domain knowledge about the specific regions and product categories to interpret the forecasting results and identify potential drivers of sales revenue that were not included in the model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ARIMA\n",
        "\n",
        "12 month Holdout\n"
      ],
      "metadata": {
        "id": "Gam-L0TadIOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for ARIMA\n",
        "arima_data = {}\n",
        "date_threshold_arima = '2019-09-23'\n",
        "\n",
        "for name, df in dataframes.items():\n",
        "    # Set 'Date' as index\n",
        "    df_arima = df.set_index('Date').sort_index()\n",
        "\n",
        "    # Select only the target variable for ARIMA\n",
        "    ts_arima = df_arima['Sales Revenue (USD)']\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    train_arima = ts_arima[ts_arima.index <= date_threshold_arima]\n",
        "    test_arima = ts_arima[ts_arima.index > date_threshold_arima]\n",
        "\n",
        "    arima_data[name] = {\n",
        "        'train': train_arima,\n",
        "        'test': test_arima\n",
        "    }\n",
        "\n",
        "# Display the keys of the new dictionary and the head of one of the training sets as a check\n",
        "print(\"Prepared data for ARIMA:\")\n",
        "print(arima_data.keys())\n",
        "print(\"\\nHead of one training set:\")\n",
        "display(list(arima_data.values())[0]['train'].head())\n",
        "print(\"\\nHead of one testing set:\")\n",
        "display(list(arima_data.values())[0]['test'].head())"
      ],
      "metadata": {
        "id": "K-Z-D35PdSxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0775c73"
      },
      "source": [
        "## Train ARIMA models\n",
        "\n",
        "### Subtask:\n",
        "Train an ARIMA model on the training data for each dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cdf87b7"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the prepared ARIMA data for each dataframe and train an ARIMA model on the training data. Since the optimal ARIMA parameters (p, d, q) can vary for each time series, a simple approach is to start with a common order (e.g., (5,1,0)) or implement a more sophisticated parameter selection process (e.g., using auto_arima or a grid search). For this task, we'll start with a basic order and note that this could be further optimized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48dd63c7"
      },
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "arima_models = {}\n",
        "# Define the specified ARIMA order (p, d, q)\n",
        "arima_order = (2, 1, 0)\n",
        "\n",
        "for name, data in arima_data.items():\n",
        "    print(f\"Training ARIMA model for {name} with order {arima_order}...\")\n",
        "    train_data = data['train']\n",
        "\n",
        "    try:\n",
        "        # Fit ARIMA model using statsmodels\n",
        "        model = ARIMA(train_data, order=arima_order)\n",
        "        model_fit = model.fit()\n",
        "        arima_models[name] = model_fit\n",
        "        print(f\"Finished training for {name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not train ARIMA model for {name}: {e}\")\n",
        "        arima_models[name] = None # Store None if training fails\n",
        "\n",
        "print(\"\\nAll ARIMA models trained (or attempted to train).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "441d5884"
      },
      "source": [
        "## Forecast future sales with ARIMA\n",
        "\n",
        "### Subtask:\n",
        "Use the trained ARIMA models to forecast sales revenue for the next 1 year for each dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce8e78d0"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the trained ARIMA models and the test data for each dataframe to generate sales revenue forecasts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "bee5f805"
      },
      "source": [
        "arima_forecasts = {}\n",
        "\n",
        "for name, model in arima_models.items():\n",
        "    if model: # Check if model training was successful\n",
        "        print(f\"Generating ARIMA forecasts for {name}...\")\n",
        "        test_data = arima_data[name]['test']\n",
        "        # Forecast for the length of the test set\n",
        "        forecast = model.forecast(steps=len(test_data))\n",
        "        arima_forecasts[name] = forecast\n",
        "        print(f\"Finished forecasting for {name}\")\n",
        "    else:\n",
        "        print(f\"Skipping forecasting for {name} due to failed model training.\")\n",
        "        arima_forecasts[name] = None\n",
        "\n",
        "print(\"\\nAll ARIMA forecasts generated (or skipped).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "406c2601"
      },
      "source": [
        "## Summarize ARIMA forecasting results\n",
        "\n",
        "### Subtask:\n",
        "Calculate and summarize the performance metrics (MAE, RMSE, MAPE) for the ARIMA forecasts and discuss the key findings and potential next steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "998a69a0"
      },
      "source": [
        "**Reasoning**:\n",
        "Calculate MAE, RMSE, and MAPE for the ARIMA forecasts for each dataframe and display the results in a table. Discuss the performance of the ARIMA models and suggest next steps, including comparing the ARIMA results to the XGBoost results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcc2d392"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "arima_results_summary = {}\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    \"\"\"Calculates Mean Absolute Percentage Error (MAPE).\"\"\"\n",
        "    # Avoid division by zero for actual values that are zero\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    # Replace 0s in y_true with a small epsilon to avoid division by zero\n",
        "    y_true = np.where(y_true == 0, np.finfo(float).eps, y_true)\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "for name in arima_forecasts.keys():\n",
        "    if arima_forecasts[name] is not None:\n",
        "        y_true = arima_data[name]['test']\n",
        "        y_pred_series = arima_forecasts[name]\n",
        "\n",
        "        # Explicitly set the index of the forecast to match the test data index\n",
        "        y_pred_series.index = y_true.index\n",
        "\n",
        "        # Align the series based on their index (which should now match)\n",
        "        y_true_aligned, y_pred_aligned = y_true.align(y_pred_series, join='inner')\n",
        "\n",
        "        # Ensure there are samples after alignment\n",
        "        if len(y_true_aligned) > 0:\n",
        "            mae = mean_absolute_error(y_true_aligned, y_pred_aligned)\n",
        "            rmse = np.sqrt(mean_squared_error(y_true_aligned, y_pred_aligned))\n",
        "            mape = mean_absolute_percentage_error(y_true_aligned, y_pred_aligned)\n",
        "\n",
        "            arima_results_summary[name] = {\n",
        "                'ARIMA MAE': mae,\n",
        "                'ARIMA RMSE': rmse,\n",
        "                'ARIMA MAPE': mape\n",
        "            }\n",
        "        else:\n",
        "            print(f\"No overlapping dates for {name} after alignment. Skipping metrics calculation.\")\n",
        "            arima_results_summary[name] = {\n",
        "                'ARIMA MAE': np.nan,\n",
        "                'ARIMA RMSE': np.nan,\n",
        "                'ARIMA MAPE': np.nan\n",
        "            }\n",
        "    else:\n",
        "         arima_results_summary[name] = {\n",
        "            'ARIMA MAE': np.nan,\n",
        "            'ARIMA RMSE': np.nan,\n",
        "            'ARIMA MAPE': np.nan\n",
        "        }\n",
        "\n",
        "\n",
        "arima_summary_df = pd.DataFrame.from_dict(arima_results_summary, orient='index')\n",
        "\n",
        "print(\"Summary of ARIMA Forecasting Performance (MAE, RMSE, and MAPE):\")\n",
        "display(arima_summary_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc93ad8b"
      },
      "source": [
        "## Comparison and Summary\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "* ARIMA models with a fixed order (2, 1, 0) were trained on the time series data for each of the 24 Region and Product Category combinations.\n",
        "* Sales revenue forecasts for the test period (next year based on the split) were generated using the trained ARIMA models.\n",
        "* 95% confidence intervals for the ARIMA forecasts were calculated.\n",
        "* Visualizations were created for each dataset, displaying the historical data, actual future data, ARIMA forecasted sales revenue, and the calculated confidence intervals.\n",
        "* Model performance was evaluated using Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE) on the test sets for each dataset.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "* **Compare ARIMA and XGBoost Performance:** Now that both XGBoost and ARIMA models have been trained and evaluated, compare the `summary_df` (XGBoost results) with the `arima_summary_df` (ARIMA results) to determine which model performed better for each specific time series. This comparison should be based on MAE, RMSE, and MAPE.\n",
        "* **Analyze Performance Differences:** Investigate why one model might outperform the other for certain datasets. ARIMA models are often good at capturing temporal dependencies, while XGBoost, being a tree-based model, might be better at capturing non-linear relationships with the features (Marketing Spend and Ad Campaign Effectiveness).\n",
        "* **Model Selection:** Based on the performance comparison, select the best model for forecasting sales revenue for each Region and Product Category combination.\n",
        "* **Ensemble Methods:** Consider combining the forecasts from both the ARIMA and XGBoost models using ensemble techniques to potentially achieve even better forecasting accuracy.\n",
        "* **Further ARIMA Tuning:** If ARIMA performance is not satisfactory, consider performing a more thorough analysis to determine the optimal (p, d, q) order for each time series. This could involve analyzing ACF/PACF plots, using information criteria (AIC, BIC), or implementing a grid search.\n",
        "* **Review Confidence Intervals:** Examine the confidence intervals from both models in the visualizations. Wider intervals indicate greater uncertainty in the forecasts. Analyze if the actual future data falls within the confidence intervals for both models.\n",
        "* **External Factors:** Revisit the possibility of incorporating external factors (beyond Marketing Spend and Ad Campaign Effectiveness) that might influence sales revenue, such as economic indicators, competitor actions, or seasonal events, which could improve the performance of both models.\n",
        "\n",
        "**Finish task**: Based on the comparison and further analysis, finalize the sales revenue forecasts and present the findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d3462b3"
      },
      "source": [
        "## Calculate confidence intervals for ARIMA forecasts\n",
        "\n",
        "### Subtask:\n",
        "Calculate confidence intervals for the ARIMA forecasted sales revenue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97abc539"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the trained ARIMA models and their corresponding forecasts to calculate the confidence intervals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bff9b45"
      },
      "source": [
        "arima_confidence_intervals = {}\n",
        "confidence_level = 0.95 # 95% confidence interval\n",
        "\n",
        "for name, model in arima_models.items():\n",
        "    if model: # Check if model training was successful\n",
        "        print(f\"Calculating confidence intervals for {name}...\")\n",
        "        test_data = arima_data[name]['test']\n",
        "        # Get forecast and confidence intervals\n",
        "        forecast_results = model.get_forecast(steps=len(test_data))\n",
        "        confidence_int = forecast_results.conf_int(alpha=1 - confidence_level)\n",
        "\n",
        "        arima_confidence_intervals[name] = confidence_int\n",
        "        print(f\"Finished calculating confidence intervals for {name}\")\n",
        "    else:\n",
        "        print(f\"Skipping confidence interval calculation for {name} due to failed model training.\")\n",
        "        arima_confidence_intervals[name] = None\n",
        "\n",
        "print(\"\\nConfidence intervals for all ARIMA forecasts calculated (or skipped).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1641c12"
      },
      "source": [
        "## Visualize ARIMA forecasts\n",
        "\n",
        "### Subtask:\n",
        "For each dataframe, visualize the historical data, the ARIMA forecasted sales revenue, and the confidence interval."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36b8b651"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through each dataframe and plot the historical data, ARIMA forecasts, and confidence intervals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c147f09"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for name in arima_data.keys():\n",
        "    if arima_models[name] and arima_forecasts[name] is not None and arima_confidence_intervals[name] is not None:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        # Plot historical data\n",
        "        plt.plot(arima_data[name]['train'].index, arima_data[name]['train'], label='Historical Data')\n",
        "        plt.plot(arima_data[name]['test'].index, arima_data[name]['test'], label='Actual Future Data', color='orange')\n",
        "\n",
        "\n",
        "        # Plot forecast and confidence intervals\n",
        "        # Ensure the forecast and confidence interval indices match the test data index\n",
        "        forecast_index = arima_data[name]['test'].index\n",
        "        plt.plot(forecast_index, arima_forecasts[name], label='ARIMA Forecast', color='green')\n",
        "        plt.fill_between(forecast_index,\n",
        "                         arima_confidence_intervals[name].iloc[:, 0],\n",
        "                         arima_confidence_intervals[name].iloc[:, 1], color='green', alpha=.2, label='95% Confidence Interval')\n",
        "\n",
        "        plt.title(f'ARIMA Sales Revenue Forecast for {name}')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Sales Revenue (USD)')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"Skipping visualization for {name} due to failed model training or forecasting.\")\n",
        "\n",
        "print(\"\\nAll ARIMA forecasts visualized (or skipped).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ARIMAX\n",
        "\n",
        "Using Marketing Spend as Independent Variable"
      ],
      "metadata": {
        "id": "IsYjdMoRgl8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for ARIMAX\n",
        "arimax_data = {}\n",
        "date_threshold_arimax = '2019-09-23'\n",
        "\n",
        "for name, df in dataframes.items():\n",
        "    # Set 'Date' as index\n",
        "    df_arimax = df.set_index('Date').sort_index()\n",
        "\n",
        "    # Select the target variable and exogenous variable for ARIMAX\n",
        "    ts_arimax = df_arimax['Sales Revenue (USD)']\n",
        "    exog_arimax = df_arimax[['Marketing Spend (USD)']] # Using Marketing Spend as the exogenous variable\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    train_ts_arimax = ts_arimax[ts_arimax.index <= date_threshold_arimax]\n",
        "    test_ts_arimax = ts_arimax[ts_arimax.index > date_threshold_arimax]\n",
        "    train_exog_arimax = exog_arimax[exog_arimax.index <= date_threshold_arimax]\n",
        "    test_exog_arimax = exog_arimax[exog_arimax.index > date_threshold_arimax]\n",
        "\n",
        "\n",
        "    arimax_data[name] = {\n",
        "        'train_ts': train_ts_arimax,\n",
        "        'test_ts': test_ts_arimax,\n",
        "        'train_exog': train_exog_arimax,\n",
        "        'test_exog': test_exog_arimax\n",
        "    }\n",
        "\n",
        "# Display the keys of the new dictionary and the head of one of the training sets as a check\n",
        "print(\"Prepared data for ARIMAX:\")\n",
        "print(arimax_data.keys())\n",
        "print(\"\\nHead of one training set (target):\")\n",
        "display(list(arimax_data.values())[0]['train_ts'].head())\n",
        "print(\"\\nHead of one training set (exogenous):\")\n",
        "display(list(arimax_data.values())[0]['train_exog'].head())\n",
        "print(\"\\nHead of one testing set (target):\")\n",
        "display(list(arimax_data.values())[0]['test_ts'].head())\n",
        "print(\"\\nHead of one testing set (exogenous):\")\n",
        "display(list(arimax_data.values())[0]['test_exog'].head())"
      ],
      "metadata": {
        "id": "97BE-Wl9g2WS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f1d97dc"
      },
      "source": [
        "## Comparison and Summary\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "* ARIMAX models with a fixed ARIMA order (2, 1, 0) and 'Marketing Spend (USD)' as an exogenous variable were trained on the time series data for each of the 24 Region and Product Category combinations.\n",
        "* Sales revenue forecasts for the test period (next year based on the split) were generated using the trained ARIMAX models and the corresponding exogenous data.\n",
        "* 95% confidence intervals for the ARIMAX forecasts were calculated.\n",
        "* Visualizations were created for each dataset, displaying the historical data, actual future data, ARIMAX forecasted sales revenue, and the calculated confidence intervals.\n",
        "* Model performance was evaluated using Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE) on the test sets for each dataset.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "* **Compare Model Performance:** Now that forecasts from XGBoost, ARIMA, and ARIMAX models are available, compare the performance metrics (`summary_df`, `arima_summary_df`, and `arimax_summary_df`) to determine which model provides the most accurate forecasts for each specific time series. Consider MAE, RMSE, and MAPE for this comparison.\n",
        "* **Analyze Exogenous Variable Impact:** Evaluate if the inclusion of 'Marketing Spend (USD)' as an exogenous variable in the ARIMAX model improved forecasting accuracy compared to the standard ARIMA model. This can be done by comparing the performance metrics of the ARIMA and ARIMAX models.\n",
        "* **Model Selection:** Based on the comprehensive performance comparison across all three models, select the best model for forecasting sales revenue for each Region and Product Category combination.\n",
        "* **Further Model Improvement:** For the selected best model(s), consider further tuning hyperparameters, exploring additional relevant exogenous variables, or investigating more advanced time series modeling techniques if needed to improve forecasting accuracy.\n",
        "* **Review Confidence Intervals:** Examine the confidence intervals from all models in the visualizations. Wider intervals indicate greater uncertainty. Assess which model provides narrower confidence intervals while still capturing the actual future data points.\n",
        "* **Business Context:** Interpret the forecasting results within the business context of each region and product category. Consider if the forecasts align with business expectations and if there are any insights from the models that can inform business strategies related to marketing spend or other factors.\n",
        "\n",
        "**Finish task**: Based on the comparison and further analysis, finalize the sales revenue forecasts and present the findings, including a recommendation for the best forecasting approach for each time series."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5995dac"
      },
      "source": [
        "## Train ARIMAX models\n",
        "\n",
        "### Subtask:\n",
        "Train an ARIMAX model on the training data for each dataframe, using 'Marketing Spend (USD)' as the exogenous variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df53d7b7"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the prepared ARIMAX data for each dataframe and train an ARIMAX model on the training time series data with the corresponding training exogenous variable. Similar to ARIMA, the optimal ARIMAX order and exogenous variables can be complex to determine automatically without libraries like `pmdarima`. For this task, we'll use a basic ARIMA order (2, 1, 0) and the specified exogenous variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "6df83d30"
      },
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "arimax_models = {}\n",
        "# Define a basic ARIMAX order (p, d, q) - this can be optimized\n",
        "arimax_order = (2, 1, 0)\n",
        "\n",
        "for name, data in arimax_data.items():\n",
        "    print(f\"Training ARIMAX model for {name} with order {arimax_order} and exogenous variable 'Marketing Spend (USD)'...\")\n",
        "    train_ts = data['train_ts']\n",
        "    train_exog = data['train_exog']\n",
        "\n",
        "    try:\n",
        "        # Fit ARIMAX model using statsmodels\n",
        "        model = ARIMA(train_ts, exog=train_exog, order=arimax_order)\n",
        "        model_fit = model.fit()\n",
        "        arimax_models[name] = model_fit\n",
        "        print(f\"Finished training for {name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not train ARIMAX model for {name}: {e}\")\n",
        "        arimax_models[name] = None # Store None if training fails\n",
        "\n",
        "print(\"\\nAll ARIMAX models trained (or attempted to train).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce4ef901"
      },
      "source": [
        "## Forecast future sales with ARIMAX\n",
        "\n",
        "### Subtask:\n",
        "Use the trained ARIMAX models to forecast sales revenue for the next 1 year for each dataframe, using the corresponding exogenous data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "398a0bc6"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the trained ARIMAX models and the test time series and exogenous data for each dataframe to generate sales revenue forecasts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "57d4d5fe"
      },
      "source": [
        "arimax_forecasts = {}\n",
        "\n",
        "for name, model in arimax_models.items():\n",
        "    if model: # Check if model training was successful\n",
        "        print(f\"Generating ARIMAX forecasts for {name}...\")\n",
        "        test_ts = arimax_data[name]['test_ts']\n",
        "        test_exog = arimax_data[name]['test_exog']\n",
        "\n",
        "        # Forecast for the length of the test set using the test exogenous data\n",
        "        forecast = model.forecast(steps=len(test_ts), exog=test_exog)\n",
        "        arimax_forecasts[name] = forecast\n",
        "        print(f\"Finished forecasting for {name}\")\n",
        "    else:\n",
        "        print(f\"Skipping forecasting for {name} due to failed model training.\")\n",
        "        arimax_forecasts[name] = None\n",
        "\n",
        "print(\"\\nAll ARIMAX forecasts generated (or skipped).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a86cfce"
      },
      "source": [
        "## Calculate confidence intervals for ARIMAX forecasts\n",
        "\n",
        "### Subtask:\n",
        "Calculate confidence intervals for the ARIMAX forecasted sales revenue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b9d8a87"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the trained ARIMAX models and their corresponding forecasts to calculate the confidence intervals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "d9c1fa85"
      },
      "source": [
        "arimax_confidence_intervals = {}\n",
        "confidence_level = 0.95 # 95% confidence interval\n",
        "\n",
        "for name, model in arimax_models.items():\n",
        "    if model: # Check if model training was successful\n",
        "        print(f\"Calculating confidence intervals for {name}...\")\n",
        "        test_ts = arimax_data[name]['test_ts']\n",
        "        test_exog = arimax_data[name]['test_exog']\n",
        "        # Get forecast and confidence intervals\n",
        "        forecast_results = model.get_forecast(steps=len(test_ts), exog=test_exog)\n",
        "        confidence_int = forecast_results.conf_int(alpha=1 - confidence_level)\n",
        "\n",
        "        arimax_confidence_intervals[name] = confidence_int\n",
        "        print(f\"Finished calculating confidence intervals for {name}\")\n",
        "    else:\n",
        "        print(f\"Skipping confidence interval calculation for {name} due to failed model training.\")\n",
        "        arimax_confidence_intervals[name] = None\n",
        "\n",
        "print(\"\\nConfidence intervals for all ARIMAX forecasts calculated (or skipped).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ae137a9"
      },
      "source": [
        "## Visualize ARIMAX forecasts\n",
        "\n",
        "### Subtask:\n",
        "For each dataframe, visualize the historical data, the ARIMAX forecasted sales revenue, and the confidence interval."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf23cee1"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through each dataframe and plot the historical data, ARIMAX forecasts, and confidence intervals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53b8dd3d"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for name in arimax_data.keys():\n",
        "    if arimax_models[name] and arimax_forecasts[name] is not None and arimax_confidence_intervals[name] is not None:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        # Plot historical data\n",
        "        plt.plot(arimax_data[name]['train_ts'].index, arimax_data[name]['train_ts'], label='Historical Data')\n",
        "        plt.plot(arimax_data[name]['test_ts'].index, arimax_data[name]['test_ts'], label='Actual Future Data', color='orange')\n",
        "\n",
        "\n",
        "        # Plot forecast and confidence intervals\n",
        "        # Ensure the forecast and confidence interval indices match the test data index\n",
        "        forecast_index = arimax_data[name]['test_ts'].index\n",
        "        plt.plot(forecast_index, arimax_forecasts[name], label='ARIMAX Forecast', color='green')\n",
        "        plt.fill_between(forecast_index,\n",
        "                         arimax_confidence_intervals[name].iloc[:, 0],\n",
        "                         arimax_confidence_intervals[name].iloc[:, 1], color='green', alpha=.2, label='95% Confidence Interval')\n",
        "\n",
        "        plt.title(f'ARIMAX Sales Revenue Forecast for {name}')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Sales Revenue (USD)')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "plt.close()\n",
        "\n",
        "    else:\n",
        "        print(f\"Skipping visualization for {name} due to failed model training or forecasting.\")\n",
        "\n",
        "print(\"\\nAll ARIMAX forecasts visualized (or skipped).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cddb9e5"
      },
      "source": [
        "## Summarize ARIMAX forecasting results\n",
        "\n",
        "### Subtask:\n",
        "Calculate and summarize the performance metrics (MAE, RMSE, MAPE) for the ARIMAX forecasts and discuss the key findings and potential next steps, including comparison with other models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f27041f"
      },
      "source": [
        "**Reasoning**:\n",
        "Calculate MAE, RMSE, and MAPE for the ARIMAX forecasts for each dataframe and display the results in a table. Discuss the performance of the ARIMAX models and suggest next steps, including comparing the ARIMAX results to the ARIMA and XGBoost results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1bb2ab4"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "arimax_results_summary = {}\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    \"\"\"Calculates Mean Absolute Percentage Error (MAPE).\"\"\"\n",
        "    # Avoid division by zero for actual values that are zero\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    # Replace 0s in y_true with a small epsilon to avoid division by zero\n",
        "    y_true = np.where(y_true == 0, np.finfo(float).eps, y_true)\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "for name in arimax_forecasts.keys():\n",
        "    if arimax_forecasts[name] is not None:\n",
        "        y_true = arimax_data[name]['test_ts']\n",
        "        y_pred_series = arimax_forecasts[name]\n",
        "\n",
        "        # Explicitly set the index of the forecast to match the test data index\n",
        "        y_pred_series.index = y_true.index\n",
        "\n",
        "        # Align the series based on their index (which should now match)\n",
        "        y_true_aligned, y_pred_aligned = y_true.align(y_pred_series, join='inner')\n",
        "\n",
        "        # Ensure there are samples after alignment\n",
        "        if len(y_true_aligned) > 0:\n",
        "            mae = mean_absolute_error(y_true_aligned, y_pred_aligned)\n",
        "            rmse = np.sqrt(mean_squared_error(y_true_aligned, y_pred_aligned))\n",
        "            mape = mean_absolute_percentage_error(y_true_aligned, y_pred_aligned)\n",
        "\n",
        "            arimax_results_summary[name] = {\n",
        "                'ARIMAX MAE': mae,\n",
        "                'ARIMAX RMSE': rmse,\n",
        "                'ARIMAX MAPE': mape\n",
        "            }\n",
        "        else:\n",
        "            print(f\"No overlapping dates for {name} after alignment. Skipping metrics calculation.\")\n",
        "            arimax_results_summary[name] = {\n",
        "                'ARIMAX MAE': np.nan,\n",
        "                'ARIMAX RMSE': np.nan,\n",
        "                'ARIMAX MAPE': np.nan\n",
        "            }\n",
        "    else:\n",
        "         arimax_results_summary[name] = {\n",
        "            'ARIMAX MAE': np.nan,\n",
        "            'ARIMAX RMSE': np.nan,\n",
        "            'ARIMAX MAPE': np.nan\n",
        "        }\n",
        "\n",
        "\n",
        "arimax_summary_df = pd.DataFrame.from_dict(arimax_results_summary, orient='index')\n",
        "\n",
        "print(\"Summary of ARIMAX Forecasting Performance (MAE, RMSE, and MAPE):\")\n",
        "display(arimax_summary_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SUMMARY RESULTS AND DATA ANALYSIS FINDING"
      ],
      "metadata": {
        "id": "gQ4HTW3dichC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all summary dataframes\n",
        "combined_summary_df = summary_df.join(arima_summary_df).join(arimax_summary_df)\n",
        "\n",
        "# Add AIC and BIC from ARIMA and ARIMAX models\n",
        "aic_bic_results = {}\n",
        "\n",
        "for name, model in arima_models.items():\n",
        "    if model:\n",
        "        aic_bic_results[name] = {\n",
        "            'ARIMA AIC': model.aic,\n",
        "            'ARIMA BIC': model.bic\n",
        "        }\n",
        "    else:\n",
        "        aic_bic_results[name] = {\n",
        "            'ARIMA AIC': np.nan,\n",
        "            'ARIMA BIC': np.nan\n",
        "        }\n",
        "\n",
        "for name, model in arimax_models.items():\n",
        "    if model:\n",
        "        # ARIMAX models from statsmodels have AIC and BIC attributes\n",
        "        aic_bic_results[name]['ARIMAX AIC'] = model.aic\n",
        "        aic_bic_results[name]['ARIMAX BIC'] = model.bic\n",
        "    else:\n",
        "        aic_bic_results[name]['ARIMAX AIC'] = np.nan\n",
        "        aic_bic_results[name]['ARIMAX BIC'] = np.nan\n",
        "\n",
        "\n",
        "aic_bic_df = pd.DataFrame.from_dict(aic_bic_results, orient='index')\n",
        "\n",
        "# Join the AIC and BIC results to the combined summary\n",
        "final_summary_df = combined_summary_df.join(aic_bic_df)\n",
        "\n",
        "print(\"Comprehensive Model Performance Summary:\")\n",
        "display(final_summary_df)"
      ],
      "metadata": {
        "id": "6rIP6ZSAihS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "234055a9"
      },
      "source": [
        "## Model Comparison Summary for Project Submission\n",
        "\n",
        "This project explored three different modeling approaches to forecast sales revenue: XGBoost, ARIMA, and ARIMAX. Each model was trained and evaluated on historical sales data, and their performance was compared using various metrics.\n",
        "\n",
        "**1. XGBoost Model**\n",
        "\n",
        "*   **Type:** Tree-based ensemble model.\n",
        "*   **Dependent Variable:** Sales Revenue (USD).\n",
        "*   **Independent Variables:** Marketing Spend (USD) and Ad Campaign Effectiveness.\n",
        "*   **Approach:** Trained on the historical data with Marketing Spend and Ad Campaign Effectiveness as features to predict Sales Revenue. Forecasts were generated using the trained model and future values of the independent variables.\n",
        "*   **Performance:** Performance was evaluated using Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE) on the test set. The `summary_df` table provides a detailed breakdown of these metrics for each Region and Product Category combination.\n",
        "\n",
        "**2. ARIMA Model**\n",
        "\n",
        "*   **Type:** Autoregressive Integrated Moving Average (ARIMA) time series model.\n",
        "*   **Dependent Variable:** Sales Revenue (USD).\n",
        "*   **Independent Variables:** None (univariate time series model).\n",
        "*   **Order:** A fixed order of (2, 1, 0) was used for all time series. This order was chosen as a starting point and could potentially be further optimized for individual time series.\n",
        "*   **Approach:** Trained on the historical time series data of Sales Revenue. Forecasts were generated based on the historical patterns in the data.\n",
        "*   **Performance:** Performance was evaluated using MAE, RMSE, and MAPE on the test set. The `arima_summary_df` table shows these metrics. Additionally, AIC and BIC were calculated for each trained ARIMA model, providing insights into the model's fit (lower values generally indicate a better fit). These values are included in the `final_summary_df` table.\n",
        "\n",
        "**3. ARIMAX Model**\n",
        "\n",
        "*   **Type:** Autoregressive Integrated Moving Average with Exogenous Variables (ARIMAX) time series model.\n",
        "*   **Dependent Variable:** Sales Revenue (USD).\n",
        "*   **Independent Variable (Exogenous):** Marketing Spend (USD).\n",
        "*   **Order:** A fixed ARIMA order of (2, 1, 0) was used, with 'Marketing Spend (USD)' as the exogenous variable.\n",
        "*   **Approach:** Trained on the historical time series data of Sales Revenue, incorporating 'Marketing Spend (USD)' as an external factor influencing sales. Forecasts were generated using the trained model and the future values of 'Marketing Spend (USD)' for the forecast period.\n",
        "*   **Performance:** Performance was evaluated using MAE, RMSE, and MAPE on the test set. The `arimax_summary_df` table presents these metrics. AIC and BIC were also calculated for each trained ARIMAX model, and these are included in the `final_summary_df` table.\n",
        "\n",
        "**Comparison and Conclusion**\n",
        "\n",
        "The `final_summary_df` table provides a direct comparison of the MAE, RMSE, and MAPE for all three models across each of the 24 Region and Product Category combinations. By examining this table, we can assess which model performed best for each specific time series based on these error metrics (lower values indicate better performance).\n",
        "\n",
        "Comparing the ARIMA and ARIMAX results helps to understand the impact of including 'Marketing Spend (USD)' as an exogenous variable. If the ARIMAX model shows significantly better performance metrics and lower AIC/BIC values than the ARIMA model for a particular time series, it suggests that Marketing Spend is a valuable predictor of sales revenue for that category/region.\n",
        "\n",
        "Comparing the time series models (ARIMA and ARIMAX) with the machine learning model (XGBoost) provides insights into whether capturing temporal dependencies alone (ARIMA), temporal dependencies with an external factor (ARIMAX), or a more flexible machine learning approach with potentially non-linear relationships (XGBoost) is more effective for forecasting sales revenue in different scenarios.\n",
        "\n",
        "Based on the performance metrics in the `final_summary_df` table, you can identify the most accurate model for each specific Region and Product Category. This information can then be used to inform future forecasting efforts and potentially guide business decisions related to marketing spend and other factors.\n",
        "\n",
        "**Potential Next Steps**\n",
        "\n",
        "*   **Analyze `final_summary_df`:** Conduct a detailed analysis of the `final_summary_df` to identify which model performed best (lowest MAE, RMSE, MAPE, AIC, BIC) for each specific Region and Product Category.\n",
        "*   **Investigate Performance Discrepancies:** For cases where one model significantly outperforms the others, delve deeper into the characteristics of that time series and the models to understand why.\n",
        "*   **Optimal ARIMA/ARIMAX Order:** If time permits and forecasting accuracy is critical, consider implementing a more rigorous method for determining the optimal ARIMA/ARIMAX order for each time series (e.g., grid search over a range of p, d, q values) instead of using a fixed order.\n",
        "*   **Ensemble Modeling:** Explore creating ensemble models that combine the forecasts from the best-performing models to potentially achieve even greater accuracy.\n",
        "*   **Incorporate More Exogenous Variables:** If other relevant external factors are available (e.g., economic indicators, competitor data, seasonal dummy variables), consider including them in the ARIMAX or XGBoost models to potentially improve forecasts."
      ]
    }
  ]
}